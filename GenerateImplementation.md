Alright, letâ€™s break this down step-by-step, **with your exact stack in mind**:

* **Gemini** = LLM for Q\&A and generation
* **Pinecone** = vector DB for embeddings + retrieval
* **Next.js frontend** (assumed) + API routes

Iâ€™ll cover **frontend flow**, **backend flow**, **prompt logic**, and **integration glue** â€” in baby-simple, over-specific detail.

---

# ğŸ“˜ Frontend (What user sees & clicks)

1. **Generate Button (Top of Doc Page)**

   * Add a button labeled **Generate** in the top bar.
   * When clicked, it shows 3 options: **Summary / Letter / Report**.
   * User picks one.

2. **Chat Box (Extra Options)**

   * Inside chat input, add 3 small icons (Summary / Letter / Report).
   * If clicked, we also pass the **chat history** along with the document context.

3. **Visual Indicator**

   * When generation is running, show a **blue glowing icon** (like GPT tool toggle).
   * While waiting, show spinner with â€œGenerating \[Summary/Letter/Report]â€¦â€

4. **Output Display**

   * When ready, show the output in a modal or right panel.
   * At the top:

     * â€œğŸ“‘ Generated from: Documentâ€
     * or â€œğŸ“‘ Generated from: Document + Chat Contextâ€
   * Buttons: **Copy** + **Download as PDF/Docx**

---

# âš™ï¸ Backend (What happens behind the scenes)

## Step 1. User Action â†’ API Call

* Frontend calls an endpoint:

  ```
  POST /api/generate
  {
    "docId": "123",
    "type": "letter",   // or "summary" / "report"
    "conversationId": "abc" // optional, only if triggered in chat
  }
  ```

## Step 2. Retrieve Context

* API fetches top chunks from **Pinecone** for the document:

  * `namespace = userId`
  * `filter = { docId }`
  * Query = â€œauto-summaryâ€ prompt or userâ€™s last chat question (if context is included).
* Returns \~5â€“10 relevant chunks (with text + page numbers).

## Step 3. Build Prompt (based on type)

* **Summary Template:**
  â€œSummarize the following document in clear bullet points. Cite page numbers.â€

* **Letter Template:**
  â€œWrite a formal letter using the following document as source. Start with â€˜Dear \[Recipient],â€™. Keep it professional. Reference key points with page numbers.â€

* **Report Template:**
  â€œGenerate a structured report with sections: Introduction, Key Findings, Risks/Obligations, Conclusion. Ground all content in the document. Add citations.â€

*(If chat context is included: add the last N messages before the chunks.)*

## Step 4. Call Gemini

* Send `{ prompt, chunks, chatContext }` to Gemini API.
* Include system message: â€œAlways cite page numbers. If not found in context, say â€˜Not available in this document.â€™â€

## Step 5. Return Response

* API returns:

  ```
  {
    "output": "Dear Sir/Madam, Based on GDPR Article 17â€¦",
    "citations": [
      { "page": 17, "text": "Right to erasureâ€¦" }
    ]
  }
  ```

---

# ğŸ§  Technical Glue

* **Embeddings**: When ingesting PDFs, store embeddings in Pinecone with metadata:

  ```
  {
    docId: "123",
    page: 17,
    userId: "xyz",
    text: "Right to be forgottenâ€¦"
  }
  ```
* **Retrieval**: When generating, query Pinecone â†’ get top K chunks â†’ add to prompt.
* **Gemini**: Wrap Gemini call in a helper function, include both **retrieved chunks** and **user context**.
* **Credits**: Deduct credits when `/generate` runs (5 credits).
* **Output Save**: Save generated text in DB under `generations` table (optional, for history).

---

# âœ… Example Flow (Summary Generation)

1. User clicks **Generate â†’ Summary** on GDPR.pdf.
2. API retrieves top 10 chunks from Pinecone.
3. API builds summary prompt.
4. Sends to Gemini â†’ gets 4-bullet summary with citations.
5. App shows result in modal with â€œğŸ“‘ Generated from: Document.â€
6. Deduct 5 credits.
7. User can copy/download.

---

# ğŸ¯ Why This Is Good

* **Clean UX**: one button, three options.
* **Trust**: always cites doc pages.
* **Flexibility**: doc-only or doc+chat depending where user clicked.
* **Reusability**: same pipeline, just different prompt templates.
* **Scalable**: Pinecone stores all embeddings, Gemini handles reasoning.

---

